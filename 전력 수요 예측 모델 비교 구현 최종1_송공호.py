# -*- coding: utf-8 -*-
"""
ê³¼ì œëª…: ë‹¤ì–‘í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í™œìš©í•œ ì „ë ¥ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸ êµ¬í˜„
- Individual Household Electric Power Consumption ë°ì´í„°ì…‹ í™œìš©
- Global_active_power ì»¬ëŸ¼ ê¸°ë°˜ ì¼ë³„ í‰ê·  ì „ë ¥ ì˜ˆì¸¡
- 80% í•™ìŠµ / 20% í…ŒìŠ¤íŠ¸, ì‹œê³„ì—´ ëª¨ë¸ 4ì¢…(LSTM, GRU, BiLSTM, CNN-LSTM)
- ì„±ëŠ¥ í‰ê°€: MAE, RMSE
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import time

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì „ì²˜ë¦¬
def load_and_preprocess(file_path='household_power_consumption.txt'):
    df = pd.read_csv(
        file_path,
        sep=';',
        parse_dates={'Datetime': ['Date', 'Time']},
        dayfirst=True,
        na_values='?',
        low_memory=False
    )
    print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {df.shape}")
    print(df.head())

    df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')
    df.dropna(inplace=True)
    df.set_index('Datetime', inplace=True)
    daily_power = df['Global_active_power'].resample('D').mean()
    daily_power.ffill(inplace=True)  # ê²°ì¸¡ì¹˜ ì „ë°© ì±„ì›€
    return daily_power

# 2. ì‹œí€€ìŠ¤ ìƒì„±
def create_sequences(series, seq_len=30):
    scaler = MinMaxScaler()
    scaled = scaler.fit_transform(series.values.reshape(-1, 1))
    X, y = [], []
    for i in range(seq_len, len(scaled)):
        X.append(scaled[i-seq_len:i])
        y.append(scaled[i])
    return np.array(X), np.array(y), scaler

# 3. ëª¨ë¸ ì •ì˜
class LSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lstm = nn.LSTM(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)
    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1])

class GRUModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.gru = nn.GRU(1, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)
    def forward(self, x):
        out, _ = self.gru(x)
        return self.fc(out[:, -1])

class BiLSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.bilstm = nn.LSTM(1, 64, batch_first=True, bidirectional=True)
        self.fc = nn.Linear(128, 1)
    def forward(self, x):
        out, _ = self.bilstm(x)
        return self.fc(out[:, -1])

class CNNLSTMModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1d = nn.Conv1d(1, 16, kernel_size=3)
        self.pool = nn.MaxPool1d(kernel_size=2)
        self.lstm = nn.LSTM(16, 64, batch_first=True)
        self.fc = nn.Linear(64, 1)
    def forward(self, x):
        x = x.transpose(1, 2)
        x = self.pool(torch.relu(self.conv1d(x)))
        x = x.transpose(1, 2)
        out, _ = self.lstm(x)
        return self.fc(out[:, -1])

# 4. ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
def train_and_evaluate(model, train_loader, test_loader, scaler, epochs=10):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for _ in range(epochs):
        model.train()
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()

    model.eval()
    predictions, actuals = [], []
    with torch.no_grad():
        for batch_x, batch_y in test_loader:
            pred = model(batch_x)
            predictions.append(pred.numpy())
            actuals.append(batch_y.numpy())

    pred_all = np.vstack(predictions)
    actual_all = np.vstack(actuals)
    pred_inv = scaler.inverse_transform(pred_all)
    actual_inv = scaler.inverse_transform(actual_all)
    mae = mean_absolute_error(actual_inv, pred_inv)
    rmse = np.sqrt(mean_squared_error(actual_inv, pred_inv))

    return mae, rmse, actual_inv.flatten(), pred_inv.flatten()

# 5. ì „ì²´ ì‹¤í–‰
def main():
    file_path = "household_power_consumption.txt"
    data = load_and_preprocess(file_path)
    X, y, scaler = create_sequences(data)
    split = int(len(X) * 0.8)
    X_train, y_train = X[:split], y[:split]
    X_test, y_test = X[split:], y[split:]

    X_train_t = torch.tensor(X_train, dtype=torch.float32)
    y_train_t = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)
    X_test_t = torch.tensor(X_test, dtype=torch.float32)
    y_test_t = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)

    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=32)
    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=32)

    models = {
        'LSTM': LSTMModel(),
        'GRU': GRUModel(),
        'BiLSTM': BiLSTMModel(),
        'CNN-LSTM': CNNLSTMModel()
    }

    results = {}
    for name, model in models.items():
        print(f"\nğŸ”§ {name} ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        start = time.time()
        mae, rmse, actual, predicted = train_and_evaluate(model, train_loader, test_loader, scaler)
        elapsed = time.time() - start
        results[name] = (mae, rmse, elapsed)

        plt.figure(figsize=(10, 4))
        plt.plot(actual, label='ì‹¤ì œê°’')
        plt.plot(predicted, label='ì˜ˆì¸¡ê°’')
        plt.title(f"{name} ì˜ˆì¸¡ ê²°ê³¼")
        plt.legend()
        plt.tight_layout()
        plt.show()

    print("\nğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½:")
    best = min(results.items(), key=lambda x: x[1][1])[0]
    for name, (mae, rmse, t) in results.items():
        mark = "âœ…" if name == best else ""
        print(f"{name:10} â†’ MAE: {mae:.3f}, RMSE: {rmse:.3f}, Time: {t:.2f}s {mark}")

if __name__ == "__main__":
    main()
